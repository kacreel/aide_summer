# Class 6: Regression Trees and LLM Concepts - Lecture Guide

## Overview
Introduction to neural networks, transformers, and large language model concepts through hands-on mathematical exercises.

## Pre-Class Learning (2 hr 45 minutes)
### Required Materials:
1. **Coursera Machine Learning with Python** (45 minutes)
   - Complete Module 3 including hands-on exercises
   - Optional: Regression Trees section

2. **3Blue1Brown Neural Networks Series** (1 hr 10 minutes)
   - Visual introduction to neural network concepts
   - Mathematical intuition behind neural networks

3. **Blog Post** (40 minutes)
   - "What is ChatGPT Doingâ€¦And Why Does it Work?"
   - Understanding transformer architecture and LLM behavior

## Class Schedule

### Intro (5 minutes)
- Review pre-class materials
- Overview of neural networks and transformers

### Session 1: Perceptron by Hand (20 minutes)
**Foundational Neural Network Concepts**
- **Resource:** [W3Schools Perceptrons](https://www.w3schools.com/ai/ai_perceptrons.asp)
- Simple and straightforward introduction
- Manual calculation of perceptron outputs
- Transition to neural networks

### Session 2: Simple Neural Network Math by Hand (30 minutes)
**Manual Calculations**
- **Material:** "NeuralNet Worksheet.pdf"
- Step-by-step forward propagation
- Weight updates and backpropagation basics
- Understanding activation functions

### Break (5 minutes)

### Session 3: Transformer by Hand - Core Concepts (35 minutes)
**Attention is All You Need**
- **Primary Tutorial:** [Deep Dive into Transformers by Hand](https://docs.google.com/document/d/12Y4gtQuzSpXj-pQLKJr6SrANs_oe9uVhBzjJ2mD0zjI/edit?usp=sharing)
- **Supporting Resource:** [Matrix Multiplication Guide](https://www.mathsisfun.com/algebra/matrix-multiplying.html)
- Manual calculation of attention mechanisms
- Understanding query, key, and value matrices

### Session 4: Advanced Attention (Bonus) (20 minutes)
**For Math-Savvy Students**
- **Advanced Tutorial:** [Deep Dive into Self-Attention by Hand](https://docs.google.com/document/d/1i1XEISzYFbydbixtxZfVpTn8Q0NPoP4lKlz6e8BWEhc/edit?usp=sharing)
- **Worksheet:** [Advanced Attention Worksheet](https://drive.google.com/file/d/1HkrVexGObYxSLe13BOarCHp58b01Wn_Z/view?usp=sharing)
- **Reference:** [Keys, Queries, and Values Explanation](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)
- Detailed attention matrix calculations

### Wrap-Up (10 minutes)
- Summary of key concepts
- Connection between perceptrons, neural networks, and transformers
- Preview of next class applications

## Learning Objectives
By the end of this class, students should be able to:
- Calculate perceptron outputs manually
- Understand basic neural network forward propagation
- Perform matrix multiplication for attention mechanisms
- Explain the concept of self-attention in transformers
- Connect mathematical concepts to modern LLM architecture

## Key Concepts
- **Perceptron:** Basic building block of neural networks
- **Forward Propagation:** Information flow through neural networks
- **Attention Mechanism:** Core component of transformer architecture
- **Query, Key, Value:** Fundamental attention components
- **Self-Attention:** Mechanism for sequence understanding


## Backup Resources
- [Northeastern SEED Grant Projects](https://idi.provost.northeastern.edu/seed-grant-projects/)
- [Byron Wallace Research](https://www.byronwallace.com/)
