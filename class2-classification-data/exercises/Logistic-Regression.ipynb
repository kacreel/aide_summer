{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33a6eee9-4a74-4710-9da8-f61cc396c587"
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "# Logistic Regression with Python\n",
    "\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "    \n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "* Use Logistic Regression for classification\n",
    "* Preprocess data for modeling\n",
    "* Implement Logistic regression on real world data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62faff05-e4a9-4615-bf50-484853e8ee96"
   },
   "source": [
    "## Install and import the required libraries\n",
    "Make sure the required libraries are available by executing the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "647fb479-e4c7-497b-9716-6a482636136b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.2.0\n",
      "  Downloading numpy-2.2.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m904.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.0-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas 2.2.0 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.2.0 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pandas==2.2.3\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas==2.2.3) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas==2.2.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas==2.2.3) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas==2.2.3) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n",
      "Successfully installed pandas-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn==1.6.0\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn==1.6.0) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn==1.6.0) (1.12.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.6.0)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn==1.6.0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.0\n",
      "    Uninstalling numpy-2.2.0:\n",
      "      Successfully uninstalled numpy-2.2.0\n",
      "Successfully installed joblib-1.5.1 numpy-1.26.4 scikit-learn-1.6.0 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib==3.9.3\n",
      "  Downloading matplotlib-3.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.3)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.3)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.3)\n",
      "  Downloading fonttools-4.58.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib==3.9.3)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib==3.9.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib==3.9.3) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib==3.9.3) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.3)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib==3.9.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.9.3) (1.16.0)\n",
      "Downloading matplotlib-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.2-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.2 kiwisolver-1.4.8 matplotlib-3.9.3 pyparsing-3.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==2.2.0\n",
    "!pip install pandas==2.2.3\n",
    "!pip install scikit-learn==1.6.0\n",
    "!pip install matplotlib==3.9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a65dc016-b2da-4dfb-b329-22f0007e984e"
   },
   "source": [
    "Let's first import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca30ec97-e98e-4ab6-a7ef-c5e009347bfb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23535b9c-d771-48fa-a23c-71a931bcfe3b"
   },
   "source": [
    "## Classification with Logistic Regression\n",
    "\n",
    "### Scenario\n",
    "Assume that you are working for a telecommunications company which is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is more likely to leave the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38aebbfa-4e88-48ad-9ac1-a38d98496ccb"
   },
   "source": [
    "###  Load the Telco Churn data\n",
    "Telco Churn is a hypothetical data file that concerns a telecommunications company's efforts to reduce turnover in its customer base. Each case corresponds to a separate customer and it records various demographic and service usage information. Before you can work with the data, you must use the URL to get the ChurnData.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9173530-f8ed-499b-8888-a3755aa64b37"
   },
   "source": [
    "### About the dataset\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "<br><br>\n",
    "This data set provides you information about customer preferences, services opted, personal details, etc. which helps you predict customer churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1577f7a1-d75b-4f6c-8cfb-87da65872167"
   },
   "source": [
    "### Load Data from URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b863321-115a-44ff-9c06-79c12e8f9df0"
   },
   "outputs": [],
   "source": [
    "# churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\"\n",
    "churn_df = pd.read_csv(url)\n",
    "\n",
    "churn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2f9a9bc-0a4d-4cd5-aedf-ba870412fbbd"
   },
   "source": [
    "Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the scikit-learn algorithm:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21fc4b7a-5507-47f8-b899-4ca690a39c13"
   },
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17547154-568d-4028-94dc-ee317b28d237"
   },
   "source": [
    "For this lab, we can use a subset of the fields available to develop out model. Let us assume that the fields we use are 'tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip' and of course 'churn'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "450b6b82-bae1-425f-9b5c-7f71c643fa83"
   },
   "outputs": [],
   "source": [
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7bd34f-ad34-47b8-9690-4631eec20141"
   },
   "source": [
    "For modeling the input fields X and the target field y need to be fixed. Since that the target to be predicted is 'churn', the data under this field will be stored under the variable 'y'. We may use any combination or all of the remaining fields as the input. Store these values in the variable 'X'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "124b59a9-fb6e-497f-9b53-54e72fc33f49"
   },
   "outputs": [],
   "source": [
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "X[0:5]  #print the first 5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e00367f5-e3d0-4b29-af6a-cf6eb521c26e"
   },
   "outputs": [],
   "source": [
    "y = np.asarray(churn_df['churn'])\n",
    "y[0:5] #print the first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18abb546-c178-460f-9f2b-4d2aa43899f1"
   },
   "source": [
    "It is also a norm to standardize or normalize the dataset in order to have all the features at the same scale. This helps the model learn faster and improves the model performance. We may make use of StandardScalar function in the Scikit-Learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c64494c5-99ff-4979-aff0-6d711317a332"
   },
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit(X).transform(X)\n",
    "X_norm[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bda8bd3d-1a42-439c-8a1f-fc498e2ec1b3"
   },
   "source": [
    "### Splitting the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2168fbd0-f71a-4a7d-831b-a20012574876"
   },
   "source": [
    "The trained model has to be tested and evaluated on data which has not been used during training. Therefore, it is required to separate a part of the data for testing and the remaining for training. For this, we may make use of the train_test_split function in the scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a15ed87c-2811-47cd-bd95-632453d39496"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X_norm, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a11de30-f4c2-49b7-bf57-711a136f2fc9"
   },
   "source": [
    "## Logistic Regression Classifier modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2107919c-ab83-492d-ab06-2f57ca8cbdde"
   },
   "source": [
    "Let's build the model using __LogisticRegression__ from the Scikit-learn package and fit our model with train data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f3bf0ea-dad7-4b5d-abde-74ea79fb38e9"
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b21331-b103-4f49-baa4-25de2e695d6c"
   },
   "source": [
    "Fitting, or in simple terms training, gives us a model that has now learnt from the traning data and can be used to predict the output variable. Let us predict the churn parameter for the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0287c764-307b-49df-972d-d8960cae139d"
   },
   "outputs": [],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cd433f8-d081-4425-ac1b-5ee2aa87a705"
   },
   "source": [
    "To understand this prediction, we can also have a look at the prediction probability of data point of the test data set. Use the function __predict_proba__ , we can get the probability of each class. The first column is the probability of the record belonging to class 0, and second column that of class 1. Note that the class prediction system uses the threshold for class prediction as 0.5. This means that the class predicted is the one which is most likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "683a8f08-2b65-41c7-93d7-d977a9226ba9"
   },
   "outputs": [],
   "source": [
    "yhat_prob = LR.predict_proba(X_test)\n",
    "yhat_prob[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c050d25-dfee-4820-86c2-52f3ea7d9060"
   },
   "source": [
    "Since the purpose here is to predict the 1 class more acccurately, you can also examine what role each input feature has to play in the prediction of the 1 class. Consider the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ba2ef8e-4bd6-4609-9750-56a70be30784"
   },
   "outputs": [],
   "source": [
    "coefficients = pd.Series(LR.coef_[0], index=churn_df.columns[:-1])\n",
    "coefficients.sort_values().plot(kind='barh')\n",
    "plt.title(\"Feature Coefficients in Logistic Regression Churn Model\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a72be82-9144-48fe-9b54-7959249fd7ee"
   },
   "source": [
    "Large positive value of LR Coefficient for a given field indicates that increase in this parameter will lead to better chance of a positive, i.e. 1 class. A large negative value indicates the opposite, which means that an increase in this parameter will lead to poorer chance of a positive class. A lower absolute value indicates weaker affect of the change in that field on the predicted class. Let us examine this with the following exercises.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15aba7c0-97b8-4f6e-9f97-14314e29aca6"
   },
   "source": [
    "## Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d438a5f8-570d-4777-b14b-4032919566ac"
   },
   "source": [
    "Once the predictions have been generated, it becomes prudent to evaluate the performance of the model in predicting the target variable. Let us evaluate the log-loss value.\n",
    "\n",
    "### log loss\n",
    "\n",
    "Log loss (Logarithmic loss), also known as Binary Cross entropy loss, is a function that generates a loss value based on the class wise prediction probabilities and the actual class labels. The lower the log loss value, the better the model is considered to be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e7278f0-b4f8-476a-b7e5-e33d420071a2"
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e14ecea3-8142-4e45-9cec-741acc295b19"
   },
   "source": [
    "## Practice Exercises\n",
    "Try to attempt the following questions yourself based on what you learnt in this lab.\n",
    "\n",
    "<br>\n",
    "\n",
    "a. Let us assume we add the feature 'callcard' to the original set of input features. What will the value of log loss be in this case?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.6039104035600186.\n",
    "</details>\n",
    "\n",
    "b. Let us assume we add the feature 'wireless' to the original set of input features. What will the value of log loss be in this case?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.7227054293985518.\n",
    "</details>\n",
    "\n",
    "c. What happens to the log loss value if we add both \"callcard\" and \"wireless\" to the input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.7760557225417114\n",
    "</details>\n",
    "\n",
    "d. What happens to the log loss if we remove the feature 'equip' from the original set of input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df Make sure to edit the list of features feeding the variable X. The expected answer is 0.5302427350245369\n",
    "</details>\n",
    "\n",
    "e. What happens to the log loss if we remove the features 'income' and 'employ' from the original set of input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.6529317169884828.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cf0e278-77fd-4158-ba24-628245702cfb"
   },
   "source": [
    "### Congratulations! You're ready to move on to your next lesson!\n",
    "\n",
    "\n",
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abishek Gagneja</a>\n",
    "\n",
    "\n",
    " ### Other Contributors\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n",
    "\n",
    "<!--\n",
    "## Change Log\n",
    "\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "|2024-11-05 | 3.0 | Abhishek | Updated the descriptions, codes and lab flow |\n",
    "|2021-01-21  | 2.2  | Lakshmi  |  Updated sklearn library|\n",
    "| 2020-11-03  | 2.1  | Lakshmi  |  Updated URL of csv |\n",
    "| 2020-08-27  | 2.0  | Lavanya  |  Moved lab to course repo in GitLab |\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e488e1b2-bf5c-471d-a159-f128cb33d9ad"
   },
   "source": [
    "<h3 align=\"center\">© IBM Corporation. All rights reserved.</h3>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1NZKioEzHSSci8fYoylebnWeYDG4nS0py",
     "timestamp": 1749448392257
    },
    {
     "file_id": "13IdPvVeG0JvjMht-7yXOLUCpYAjJ_Msq",
     "timestamp": 1749448378129
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "prev_pub_hash": "c9936a3a06cf4656bb99e264a74e23001002ebb55649cddd868b26d786127876"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
