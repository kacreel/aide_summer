{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Build Your Own AI Ethics Analyzer\n",
    "\n",
    "## üìö Pre-Class Preparation (45 min)\n",
    "1. **Create Accounts** (15 min):\n",
    "   - [Hugging Face](https://huggingface.co/) - Get your API token\n",
    "   - Save your token somewhere safe - you'll need it!\n",
    "\n",
    "2. **Watch** (15 min):\n",
    "   - [Hugging Face in 15 Minutes](https://www.youtube.com/watch?v=QEaBAZQCtwE) (15 min)\n",
    "\n",
    "4. **Think About** (15 min):\n",
    "   - How might you want to use LLMs to analyze AI ethics?\n",
    "\n",
    "## üéØ What We'll Build Today\n",
    "By the end of this class, you'll have built:\n",
    "1. **Basic LLM Tools** - Text generation, summarization, Q&A\n",
    "2. **Ethics Analyzer** - A tool that analyzes text for ethical concerns\n",
    "3. **Your Own Application** - Customized for your interests\n",
    "\n",
    "## üìã Class Overview\n",
    "- **Part 1** (30 min): Baby Steps with Hugging Face\n",
    "- **Part 2** (30 min): Building Blocks - Multiple AI Tasks  \n",
    "- **Part 3** (30 min): Create Your Ethics Analyzer\n",
    "- **Part 4** (30 min): Make It Your Own\n",
    "---\n",
    "\n",
    "## üèÅ Let's Start! First, Some Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SETUP CELL - Run this first!\n",
    "print(\"üöÄ Installing required packages... (this takes 1-2 minutes)\")\n",
    "\n",
    "# Install everything we need\n",
    "!pip install -q transformers torch accelerate\n",
    "!pip install -q sentencepiece protobuf\n",
    "!pip install -q gradio  # For creating web interfaces\n",
    "\n",
    "print(\"‚úÖ Installation complete! Let's import what we need:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available (Google Colab provides free GPU!)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"üñ•Ô∏è Using: {'GPU üéÆ' if device == 0 else 'CPU üêå'}\")\n",
    "print(\"üí° Tip: If using CPU, go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Baby Steps with Hugging Face üë∂\n",
    "\n",
    "Let's start with something simple - making AI complete a sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 1.1: Your First AI Text Generation\n",
    "print(\"Loading a small, fast model...\")\n",
    "\n",
    "# We'll use GPT-2 - it's small but mighty!\n",
    "generator = pipeline('text-generation', model='gpt2', device=device)\n",
    "\n",
    "print(\"‚úÖ Model loaded! Let's generate some text:\\n\")\n",
    "\n",
    "# Try it out!\n",
    "prompt = \"The ethical implications of artificial intelligence include\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(f\"ü§ñ AI says: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 1.2: Now YOU try!\n",
    "# TODO: Change the prompt to ask about a philosophical topic you're interested in\n",
    "\n",
    "your_prompt = \"The meaning of life is\"  # ‚Üê Change this to your question!\n",
    "\n",
    "# Generate response\n",
    "your_result = generator(your_prompt, \n",
    "                       max_length=80,           # Make it longer\n",
    "                       temperature=0.8,         # 0.1 = boring, 1.0 = creative\n",
    "                       num_return_sequences=2)  # Get 2 different answers\n",
    "\n",
    "print(\"ü§ñ AI's responses:\")\n",
    "for i, response in enumerate(your_result):\n",
    "    print(f\"\\n{i+1}. {response['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Reflection Questions:\n",
    "- Was the AI's response coherent?\n",
    "- Did it show any biases?\n",
    "- How might this be useful for philosophy research?\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Building Blocks - Multiple AI Tasks üèóÔ∏è\n",
    "\n",
    "Now let's explore different AI capabilities we can use for ethics analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 2.1: Sentiment Analysis (Detecting Emotional Tone)\n",
    "print(\"Loading sentiment analyzer...\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Test statements with ethical implications\n",
    "statements = [\n",
    "    \"AI will replace all human jobs and cause mass unemployment.\",\n",
    "    \"AI can help doctors diagnose diseases more accurately.\",\n",
    "    \"Facial recognition violates privacy rights.\",\n",
    "    \"Machine learning can reduce bias in hiring decisions.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Analyzing sentiment of ethical statements:\\n\")\n",
    "for statement in statements:\n",
    "    result = sentiment_analyzer(statement)[0]\n",
    "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòü\"\n",
    "    print(f\"{emoji} {result['label']} ({result['score']:.2f}): {statement[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 2.2: Zero-Shot Classification (Categorizing Without Training)\n",
    "print(\"Loading zero-shot classifier...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", \n",
    "                     model=\"facebook/bart-large-mnli\",\n",
    "                     device=device)\n",
    "\n",
    "# Define ethical categories\n",
    "ethical_categories = [\n",
    "    \"privacy concern\",\n",
    "    \"bias and fairness\",\n",
    "    \"job displacement\", \n",
    "    \"human autonomy\",\n",
    "    \"environmental impact\",\n",
    "    \"beneficial AI\"\n",
    "]\n",
    "\n",
    "# Test text\n",
    "text = \"The AI system was found to consistently reject loan applications from minority neighborhoods.\"\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Categorizing ethical concern...\")\n",
    "result = classifier(text, candidate_labels=ethical_categories)\n",
    "\n",
    "print(f\"\\nText: '{text}'\\n\")\n",
    "print(\"Categories (sorted by relevance):\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    bar = \"‚ñà\" * int(score * 20)\n",
    "    print(f\"{label:20} {bar} {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 2.3: Question Answering\n",
    "print(\"Loading Q&A model...\")\n",
    "qa_model = pipeline(\"question-answering\", device=device)\n",
    "\n",
    "# Context about AI ethics\n",
    "context = \"\"\"\n",
    "The development of artificial intelligence raises numerous ethical concerns. \n",
    "Privacy is threatened when AI systems collect and analyze personal data without consent. \n",
    "Bias in algorithms can perpetuate discrimination against marginalized groups. \n",
    "Job displacement may occur as AI automates human tasks. \n",
    "However, AI also offers benefits like improved healthcare diagnostics and accessibility tools.\n",
    "\"\"\"\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What threatens privacy?\",\n",
    "    \"What can perpetuate discrimination?\",\n",
    "    \"What are the benefits of AI?\"\n",
    "]\n",
    "\n",
    "print(\"\\n‚ùì Asking questions about AI ethics:\\n\")\n",
    "for q in questions:\n",
    "    answer = qa_model(question=q, context=context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer['answer']} (confidence: {answer['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Your Turn: Mix and Match!\n",
    "Try combining these tools. For example:\n",
    "1. Generate text about an ethical topic\n",
    "2. Analyze its sentiment\n",
    "3. Classify what type of ethical concern it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 2.4: Combine Multiple Models\n",
    "# TODO: Complete this exercise\n",
    "\n",
    "# Step 1: Generate text about an ethical issue\n",
    "ethical_prompt = \"The use of AI in criminal justice\"  # ‚Üê Change this!\n",
    "generated = generator(ethical_prompt, max_length=100)[0]['generated_text']\n",
    "print(f\"1Ô∏è‚É£ Generated text:\\n{generated}\\n\")\n",
    "\n",
    "# Step 2: Analyze its sentiment\n",
    "# TODO: Use sentiment_analyzer on the generated text\n",
    "# sentiment = ...\n",
    "\n",
    "# Step 3: Classify the ethical concern\n",
    "# TODO: Use classifier to categorize the generated text\n",
    "# classification = ...\n",
    "\n",
    "# Print your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build Your AI Ethics Analyzer üîç‚öñÔ∏è\n",
    "\n",
    "Now let's combine everything into a powerful ethics analysis tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Building the Ethics Analyzer Class\n",
    "class AIEthicsAnalyzer:\n",
    "    def __init__(self):\n",
    "        print(\"üîß Initializing AI Ethics Analyzer...\")\n",
    "        \n",
    "        # Load all our models\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\", device=device)\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", \n",
    "                                 model=\"facebook/bart-large-mnli\", \n",
    "                                 device=device)\n",
    "        self.summarizer = pipeline(\"summarization\", \n",
    "                                 model=\"facebook/bart-large-cnn\",\n",
    "                                 device=device)\n",
    "        \n",
    "        # Ethical dimensions to analyze\n",
    "        self.ethical_dimensions = [\n",
    "            \"privacy violation\",\n",
    "            \"algorithmic bias\", \n",
    "            \"lack of transparency\",\n",
    "            \"human harm potential\",\n",
    "            \"autonomy reduction\",\n",
    "            \"fairness concern\",\n",
    "            \"accountability issue\"\n",
    "        ]\n",
    "        \n",
    "        print(\"‚úÖ Ethics Analyzer ready!\\n\")\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"Perform comprehensive ethical analysis on text\"\"\"\n",
    "        print(\"üîç Analyzing ethical implications...\\n\")\n",
    "        \n",
    "        results = {\n",
    "            \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "            \"summary\": None,\n",
    "            \"sentiment\": None,\n",
    "            \"ethical_concerns\": None,\n",
    "            \"risk_score\": None\n",
    "        }\n",
    "        \n",
    "        # 1. Summarize if text is long\n",
    "        if len(text.split()) > 50:\n",
    "            summary = self.summarizer(text, max_length=100, min_length=30)[0]\n",
    "            results[\"summary\"] = summary['summary_text']\n",
    "        \n",
    "        # 2. Analyze sentiment\n",
    "        sentiment = self.sentiment(text)[0]\n",
    "        results[\"sentiment\"] = {\n",
    "            \"label\": sentiment['label'],\n",
    "            \"score\": sentiment['score']\n",
    "        }\n",
    "        \n",
    "        # 3. Classify ethical concerns\n",
    "        ethics = self.classifier(text, candidate_labels=self.ethical_dimensions)\n",
    "        concerns = []\n",
    "        for label, score in zip(ethics['labels'][:3], ethics['scores'][:3]):\n",
    "            if score > 0.3:  # Only significant concerns\n",
    "                concerns.append({\"concern\": label, \"confidence\": score})\n",
    "        results[\"ethical_concerns\"] = concerns\n",
    "        \n",
    "        # 4. Calculate overall risk score\n",
    "        if concerns:\n",
    "            results[\"risk_score\"] = sum(c['confidence'] for c in concerns) / len(concerns)\n",
    "        else:\n",
    "            results[\"risk_score\"] = 0.0\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def print_analysis(self, results):\n",
    "        \"\"\"Pretty print the analysis results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä ETHICAL ANALYSIS REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìù Text: {results['text']}\")\n",
    "        \n",
    "        if results['summary']:\n",
    "            print(f\"\\nüìÑ Summary: {results['summary']}\")\n",
    "        \n",
    "        sent = results['sentiment']\n",
    "        emoji = \"üòä\" if sent['label'] == 'POSITIVE' else \"üòü\"\n",
    "        print(f\"\\nüí≠ Sentiment: {emoji} {sent['label']} ({sent['score']:.2%})\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  Ethical Concerns Detected:\")\n",
    "        if results['ethical_concerns']:\n",
    "            for concern in results['ethical_concerns']:\n",
    "                bar = \"üî¥\" * int(concern['confidence'] * 10)\n",
    "                print(f\"   ‚Ä¢ {concern['concern']:25} {bar} {concern['confidence']:.1%}\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ No significant ethical concerns detected\")\n",
    "        \n",
    "        # Risk level\n",
    "        risk = results['risk_score']\n",
    "        if risk > 0.7:\n",
    "            risk_level = \"üî¥ HIGH RISK\"\n",
    "        elif risk > 0.4:\n",
    "            risk_level = \"üü° MEDIUM RISK\"\n",
    "        else:\n",
    "            risk_level = \"üü¢ LOW RISK\"\n",
    "        \n",
    "        print(f\"\\nüìä Overall Ethical Risk: {risk_level} ({risk:.1%})\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Create the analyzer\n",
    "analyzer = AIEthicsAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Test the Ethics Analyzer\n",
    "test_cases = [\n",
    "    \"\"\"A tech company is using facial recognition to track employee productivity \n",
    "    by monitoring their facial expressions and eye movements throughout the workday.\"\"\",\n",
    "    \n",
    "    \"\"\"An AI tutoring system adapts to each student's learning pace and style, \n",
    "    providing personalized education to underserved communities.\"\"\",\n",
    "    \n",
    "    \"\"\"A hiring algorithm was found to penalize resumes containing names \n",
    "    common in minority communities, despite claims of being 'bias-free'.\"\"\"\n",
    "]\n",
    "\n",
    "# Analyze each case\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n\\nüîç CASE {i}:\")\n",
    "    results = analyzer.analyze(case)\n",
    "    analyzer.print_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3.1: Analyze Your Own Case\n",
    "Think of an AI application with ethical implications and analyze it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Your Turn: Analyze a real-world AI ethics case\n",
    "# TODO: Replace with your own example\n",
    "\n",
    "your_case = \"\"\"\n",
    "Describe an AI system or scenario you want to analyze for ethical concerns.\n",
    "For example: AI in healthcare, criminal justice, education, social media, etc.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze it\n",
    "your_results = analyzer.analyze(your_case)\n",
    "analyzer.print_analysis(your_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Make It Interactive with Gradio! üé®\n",
    "\n",
    "Let's create a user-friendly web interface for our analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def analyze_with_interface(text, include_summary, risk_threshold):\n",
    "    \"\"\"Gradio interface function\"\"\"\n",
    "    results = analyzer.analyze(text)\n",
    "    \n",
    "    # Format output for display\n",
    "    output = f\"### üìä Ethical Analysis Report\\n\\n\"\n",
    "    \n",
    "    # Summary (if requested)\n",
    "    if include_summary and results['summary']:\n",
    "        output += f\"**Summary:** {results['summary']}\\n\\n\"\n",
    "    \n",
    "    # Sentiment\n",
    "    sent = results['sentiment']\n",
    "    output += f\"**Sentiment:** {sent['label']} ({sent['score']:.2%})\\n\\n\"\n",
    "    \n",
    "    # Ethical concerns\n",
    "    output += \"**Ethical Concerns Detected:**\\n\"\n",
    "    if results['ethical_concerns']:\n",
    "        for concern in results['ethical_concerns']:\n",
    "            output += f\"- {concern['concern']}: {concern['confidence']:.1%}\\n\"\n",
    "    else:\n",
    "        output += \"- No significant concerns detected\\n\"\n",
    "    \n",
    "    # Risk score\n",
    "    risk = results['risk_score']\n",
    "    if risk > risk_threshold:\n",
    "        output += f\"\\n‚ö†Ô∏è **WARNING: High ethical risk detected ({risk:.1%})**\"\n",
    "    else:\n",
    "        output += f\"\\n‚úÖ **Risk Level: {risk:.1%} (below threshold)**\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=analyze_with_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=5, \n",
    "            placeholder=\"Enter text describing an AI system or scenario...\",\n",
    "            label=\"AI System Description\"\n",
    "        ),\n",
    "        gr.Checkbox(label=\"Include Summary\", value=True),\n",
    "        gr.Slider(0, 1, value=0.5, label=\"Risk Threshold\", \n",
    "                 info=\"Alert level for ethical concerns\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Analysis Results\"),\n",
    "    title=\"ü§ñ AI Ethics Analyzer\",\n",
    "    description=\"Analyze AI systems for potential ethical concerns including bias, privacy, and fairness issues.\",\n",
    "    examples=[\n",
    "        [\"An AI system screens job resumes by scanning social media profiles.\", True, 0.5],\n",
    "        [\"A healthcare AI predicts patient outcomes to prioritize treatments.\", True, 0.5],\n",
    "        [\"Schools use AI to monitor student behavior and predict dropouts.\", True, 0.3]\n",
    "    ],\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "print(\"üöÄ Launching web interface...\")\n",
    "demo.launch(share=True)  # share=True creates a public link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Extend Your Analyzer üöÄ\n",
    "\n",
    "Now let's add some advanced features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 5.1: Add Bias Detection\n",
    "class EnhancedEthicsAnalyzer(AIEthicsAnalyzer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add bias-related keywords\n",
    "        self.bias_keywords = [\n",
    "            'gender', 'race', 'age', 'ethnicity', 'religion', \n",
    "            'disability', 'sexual orientation', 'nationality',\n",
    "            'socioeconomic', 'political'\n",
    "        ]\n",
    "    \n",
    "    def detect_bias_topics(self, text):\n",
    "        \"\"\"Detect which protected groups might be affected\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        detected_groups = []\n",
    "        \n",
    "        for keyword in self.bias_keywords:\n",
    "            if keyword in text_lower:\n",
    "                detected_groups.append(keyword)\n",
    "        \n",
    "        return detected_groups\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"Enhanced analysis with bias detection\"\"\"\n",
    "        # Get base analysis\n",
    "        results = super().analyze(text)\n",
    "        \n",
    "        # Add bias detection\n",
    "        results['affected_groups'] = self.detect_bias_topics(text)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test enhanced analyzer\n",
    "enhanced_analyzer = EnhancedEthicsAnalyzer()\n",
    "\n",
    "test_text = \"\"\"The AI system showed different accuracy rates when analyzing \n",
    "resumes from different ethnic backgrounds and genders.\"\"\"\n",
    "\n",
    "results = enhanced_analyzer.analyze(test_text)\n",
    "print(f\"\\nüéØ Detected potential bias affecting: {results['affected_groups']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Exercise 5.2: Generate Ethical Recommendations\n",
    "def generate_recommendations(analysis_results):\n",
    "    \"\"\"Generate actionable recommendations based on analysis\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Check each concern and suggest mitigation\n",
    "    for concern in analysis_results.get('ethical_concerns', []):\n",
    "        if 'privacy' in concern['concern']:\n",
    "            recommendations.append(\"üîê Implement data minimization and get explicit user consent\")\n",
    "        elif 'bias' in concern['concern']:\n",
    "            recommendations.append(\"‚öñÔ∏è Audit training data and use bias mitigation techniques\")\n",
    "        elif 'transparency' in concern['concern']:\n",
    "            recommendations.append(\"üîç Provide clear explanations of AI decisions\")\n",
    "        elif 'accountability' in concern['concern']:\n",
    "            recommendations.append(\"üìã Establish clear responsibility chains and audit trails\")\n",
    "    \n",
    "    # Add group-specific recommendations\n",
    "    if analysis_results.get('affected_groups'):\n",
    "        recommendations.append(\"üë• Conduct impact assessment on affected demographic groups\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test it\n",
    "recommendations = generate_recommendations(results)\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Post-Class Project: Add RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "### What is RAG?\n",
    "RAG lets your AI reference specific documents or knowledge bases when answering questions. Think of it as giving your AI a library card!\n",
    "\n",
    "### Your Mission:\n",
    "Create an ethics analyzer that can:\n",
    "1. Store ethics guidelines and principles\n",
    "2. Reference specific ethical frameworks when analyzing\n",
    "3. Cite sources for its recommendations\n",
    "\n",
    "### Here's a starter template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ POST-CLASS PROJECT: Simple RAG Implementation\n",
    "\n",
    "# First, install additional libraries (run this in a new session)\n",
    "# !pip install sentence-transformers chromadb\n",
    "\n",
    "class SimpleRAGEthicsAnalyzer:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize embedding model\n",
    "        # from sentence_transformers import SentenceTransformer\n",
    "        # self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # TODO: Create vector database\n",
    "        # import chromadb\n",
    "        # self.client = chromadb.Client()\n",
    "        # self.collection = self.client.create_collection(\"ethics_guidelines\")\n",
    "        \n",
    "        # Sample ethics documents to store\n",
    "        self.ethics_documents = [\n",
    "            {\n",
    "                \"id\": \"1\",\n",
    "                \"title\": \"IEEE Ethics Guidelines\",\n",
    "                \"content\": \"AI systems should be transparent and explainable...\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"2\", \n",
    "                \"title\": \"EU AI Act Principles\",\n",
    "                \"content\": \"High-risk AI systems must undergo conformity assessment...\"\n",
    "            }\n",
    "            # Add more documents\n",
    "        ]\n",
    "    \n",
    "    def add_documents(self):\n",
    "        \"\"\"Add ethics documents to vector database\"\"\"\n",
    "        # TODO: Implement document storage\n",
    "        # for doc in self.ethics_documents:\n",
    "        #     embedding = self.embedder.encode(doc['content'])\n",
    "        #     self.collection.add(\n",
    "        #         embeddings=[embedding],\n",
    "        #         documents=[doc['content']],\n",
    "        #         metadatas=[{\"title\": doc['title']}],\n",
    "        #         ids=[doc['id']]\n",
    "        #     )\n",
    "        pass\n",
    "    \n",
    "    def retrieve_relevant_guidelines(self, query, k=3):\n",
    "        \"\"\"Find relevant ethics guidelines for the query\"\"\"\n",
    "        # TODO: Implement retrieval\n",
    "        # query_embedding = self.embedder.encode(query)\n",
    "        # results = self.collection.query(\n",
    "        #     query_embeddings=[query_embedding],\n",
    "        #     n_results=k\n",
    "        # )\n",
    "        # return results\n",
    "        pass\n",
    "    \n",
    "    def analyze_with_sources(self, text):\n",
    "        \"\"\"Analyze text and cite relevant guidelines\"\"\"\n",
    "        # TODO: Combine retrieval with analysis\n",
    "        # 1. Retrieve relevant guidelines\n",
    "        # 2. Use them as context for analysis  \n",
    "        # 3. Generate response with citations\n",
    "        pass\n",
    "\n",
    "# Implementation hints:\n",
    "# 1. Use sentence-transformers for embeddings\n",
    "# 2. Use ChromaDB or FAISS for vector storage\n",
    "# 3. Combine retrieved context with your prompts\n",
    "# 4. Format outputs to include source citations\n",
    "\n",
    "print(\"üéØ Challenge: Complete the RAG implementation!\")\n",
    "print(\"üìö Resources:\")\n",
    "print(\"- ChromaDB docs: https://docs.trychroma.com/\")\n",
    "print(\"- Sentence Transformers: https://www.sbert.net/\")\n",
    "print(\"- RAG tutorial: https://python.langchain.com/docs/modules/data_connection/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built your own AI Ethics Analyzer! Here's what you accomplished:\n",
    "\n",
    "### ‚úÖ Skills You've Gained:\n",
    "1. **Using Hugging Face models** for various NLP tasks\n",
    "2. **Combining multiple AI models** into a cohesive application\n",
    "3. **Analyzing ethical implications** of AI systems\n",
    "4. **Creating user interfaces** with Gradio\n",
    "5. **Understanding RAG** for enhanced AI applications\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Enhance the analyzer** with more sophisticated bias detection\n",
    "2. **Add more ethical frameworks** (deontological, virtue ethics, etc.)\n",
    "3. **Implement the RAG system** for citing ethical guidelines\n",
    "4. **Create a dataset** of AI ethics cases for testing\n",
    "5. **Deploy your tool** online using Hugging Face Spaces\n",
    "\n",
    "### üìö Resources for Continued Learning:\n",
    "- [Hugging Face Course](https://huggingface.co/course/chapter1/1)\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "\n",
    "### üí≠ Final Reflection:\n",
    "How can we ensure AI systems like the one you built are themselves ethical and unbiased?\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** You now have the power to build AI applications. Use it responsibly! üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
