{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Capstone Project - Using the Huggingface API (with Philosophy Flavor)\n",
        "In this project we'll use the [huggingface inference API Client](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) to access powerful machine learning models. You will need to [follow the directions here](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token) to sign up for a huggingface account and get an API token. Then, paste your token into the box below."
      ],
      "metadata": {
        "id": "RiqwMu0nORGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_TOKEN = \"paste your token here\""
      ],
      "metadata": {
        "id": "lJ2EnAG5POit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll install and import the required packages."
      ],
      "metadata": {
        "id": "vqzWH03EPrbx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hmarzo8Xduc",
        "outputId": "2d947f8e-66c1-4db3-cfc0-7ddde74c8d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import InferenceClient\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SCKfLnEpXmqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll need a dataset to work with. We'll import it using the Huggingface datasets library."
      ],
      "metadata": {
        "id": "yhB3-S7vP2Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"valurank/News_Articles_Categorization\")\n",
        "news_df = pd.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "p-bI6zkPMLSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaxUnrRE5pnS",
        "outputId": "b7d7a979-577c-4c88-abc6-fa1c100bae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text       Category\n",
            "0  Elon Musk, Amber Heard Something's Fishy On Wr...  Entertainment\n",
            "1  Scientists are developing more than 100 corona...        science\n",
            "2  Jared Fogle Shut Down By Judge In Bid for Earl...  Entertainment\n",
            "3  The agency had come under fire from members of...         Health\n",
            "4  Credit...Jim Wilson/The New York TimesJune 30,...       Politics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the above code as a starting point to import [this dataset](https://huggingface.co/datasets/AiresPucrs/stanford-encyclopedia-philosophy) with texts from the Stanford Encyclopedia of Philosophy. Call this dataset \"phil_df\"."
      ],
      "metadata": {
        "id": "xjJ4RXU0cGx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: Write new code to import the philosophy dataset. You can find the code to import the dataset by clicking the \"Use in Datasets Library\" button on the huggingface website.\n",
        "#you will need to use pandas to combine all of the strings for each category, for example all rows where category = 'abduction' should be collapsed to one row which concatenates all the text in the 'text' column\n",
        "#(the .groupby() and .apply() methods may be helpful here)"
      ],
      "metadata": {
        "id": "hwXiQQoD3sEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will work with some models. Huggingface provides a method where you can plug in a [huggingface task](https://huggingface.co/tasks) and get a recommended model. The tasks are defined by what you input into the model and what you want the output to be. You can try other models as well, but this one should work reliably for the task at hand."
      ],
      "metadata": {
        "id": "Zi27wS4iQdB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InferenceClient.get_recommended_model(\"text-to-image\") #use this method to get reccomended models for your tasks https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.get_recommended_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HKGWIqTj2AHu",
        "outputId": "bca8d705-626e-4168-e500-8b673b75d8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CompVis/stable-diffusion-v1-4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use the inference client to summarize some text. Notice that you have to plug in your token to use the API."
      ],
      "metadata": {
        "id": "NAOD3Ry3Q4jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summarize a text\n",
        "summarizer = InferenceClient(model=\"sshleifer/distilbart-cnn-12-6\", token = YOUR_TOKEN)\n",
        "#basic info about summary with InferenceClient https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.summarization\n",
        "#detailed parameters for summarization https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task\n",
        "print(news_df[\"Text\"][0])\n",
        "print(summarizer.summarization(news_df[\"Text\"][0]).summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TJaGO2f3dyl",
        "outputId": "7fa12aec-5e75-4270-c78c-0f8f7d9259ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk, Amber Heard Something's Fishy On Wrapped-Up Sushi Last we heard, Elon Musk and Amber Heard were \"not back together\" even though they kiss goodbye and dance real close ... sorry, we're not buying that now. Amber and Elon went on a sushi date Monday in WeHo, and looked like the full-blown hand-holding couple that's definitely on again. But that's only because that's exactly what they are -- no matter how many times they try to say they're not reunited. We broke the story ... Elon and Amber started hanging out again this past fall ... after announcing their split in the summer. Since then, they've smooched and gone dancing together. If it looks like a reunited duck, walks like a reunited duck ... TMZ.com\n",
            " Elon Musk and Amber Heard went on a sushi date Monday in WeHo . The two started hanging out again this past fall ... after announcing their split in the summer . Since then, they've smooched and gone dancing together . If it looks like a reunited duck, walks like a reunite duck .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the above code as a starting point to summarize an article from the Stanford Encyclopedia of Philosophy instead of the news articles dataset. Try changing the parameters of the model using [these detailed parameters](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task)."
      ],
      "metadata": {
        "id": "ZMsHUohLUQsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: Summarize a philosophy text"
      ],
      "metadata": {
        "id": "DlCeVt3DV4no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will answer a question based on a document. Use what you learned above and [the documentation about question answering here](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.question_answering) to answer the question \"What is a zombie?\" based on the \"Zombies\" entry in the stanford encyclopedia of philosophy."
      ],
      "metadata": {
        "id": "m33kvz90Zm0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO:answer a question based on an entry in the stanford encyclopedia of philosophy"
      ],
      "metadata": {
        "id": "slMeb38y60JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now choose another task from the [available huggingface tasks](https://huggingface.co/tasks) and implement it. One suggestion is to generate an image using the text-to-image task, but the sky is the limit! \\(Note: saved files can be found by clicking the folder icon in the toolbar on the left)"
      ],
      "metadata": {
        "id": "xbNKCBFCa9Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: Implement your task"
      ],
      "metadata": {
        "id": "XcvZdRoL9RCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}